{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESDL Project: Modelling biomass with Deep Learning from land use and ESDL variables\n",
    "## ideas\n",
    "* linking land use layers with ESDL variables for modelling biomass\n",
    "* training and testing neuronal network for estimating above ground biomass (reference data GlobBiomass 2010 is provided)\n",
    "* testing for different world regions and different variable ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading modules/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import datetime64\n",
    "from ipywidgets import interact \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from sklearn.metrics import r2_score #various classification, regression and clustering algorithms. We use for metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns # data visualization library based on matplotlib\n",
    "import tensorflow as tf #dataflow and differentiable programming (machine learning applications)\n",
    "from tensorflow import keras #neural-network building blocks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import os #operation system interface \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list of variables\n",
    "evaporation, potential_evaporation, bare_soil_evaporation, root_moisture, surface_moisture,\n",
    "land_surface_temperature, soil_moisture, white_sky_albedo, black_sky_albedo, net_ecosystem_exchange, terrestrial_ecosystem_respiration, gross_primary_productivity, precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist = [\"evaporation\", \"bare_soil_evaporation\", \"surface_moisture\",\n",
    "           \"land_surface_temperature\", \"root_moisture\", \"black_sky_albedo\", \n",
    "           \"net_ecosystem_exchange\", \"terrestrial_ecosystem_respiration\", \n",
    "           \"gross_primary_productivity\", \"leaf_area_index\", \"precipitation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESDC_img = xr.open_zarr(\"/home/jovyan/work/datacube/ESDCv2.0.0/esdc-8d-0.083deg-1x2160x4320-2.0.0.zarr\")\n",
    "ESDC_time = xr.open_zarr(\"/home/jovyan/work/datacube/ESDCv2.0.0/esdc-8d-0.083deg-184x270x270-2.0.0.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built yearly mean of 2010 for all selected variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build subset of data cube for year 2010 for Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define world region for biomass modelling\n",
    "# Europe: lat 70, 30, lon -20, 35\n",
    "# Asia: Lat 70, 5 Lon 35, 150\n",
    "# Africa: Lat 30,-35 Lon -20, 50\n",
    "# N-C-America: Lat 70,10 Lon -150,-60\n",
    "# S-America: Lat 10,-55 Lon -90,-35\n",
    "# SE-Asia-Aus: Lat 10,-40 Lon 95, 180\n",
    "\n",
    "region = \"Europe\"\n",
    "lat1 = 70\n",
    "lat2 = 30\n",
    "lon1 = -20\n",
    "lon2 = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESDC2010 = ESDC_time.sel(time = slice('2010-01-01','2010-12-31'), lat = slice(lat1,lat2), lon = slice(lon1,lon2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot mean for selected variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in varlist:\n",
    "    print(var)\n",
    "    exec(var+\"map\" + \"= ESDC2010.\" + var + \".mean(dim='time')\")\n",
    "    #exec(var+\"map\" + \".plot.pcolormesh(figsize=(16,10))\")\n",
    "    #plt.savefig(var + \"_mean_2010_map.png\",dpi = 300)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open additional files \n",
    "biomass map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_2010 = xr.open_rasterio(\"/home/jovyan/work/workspace/GlobBiomass_2010/GlobBiomass_2010_agb_0.083_avg_float.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_2010 = biomass_2010.sel(y = slice(lat1,lat2), x = slice(lon1,lon2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_2010[0,:,:].plot.pcolormesh(figsize=(16,10))\n",
    "plt.title(\"Reference AGB 2010\")\n",
    "plt.savefig(\"biomass_2010_map_\" + region + \".png\",dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load country map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = xr.open_rasterio(\"/home/jovyan/work/workspace/Country_mask_2015/country_map_2015_0.0833_mod.tif\")\n",
    "countries = countries.sel(y = slice(lat1,lat2), x = slice(lon1,lon2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load land use fractions from land use change reconstruction HILDA+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_frac_2010 = xr.open_rasterio(\"/home/jovyan/work/workspace/HILDA+_LU_2010/hilda_plus_2010_class1_frac_0.083_avg.tif\")\n",
    "crop_frac_2010 = xr.open_rasterio(\"/home/jovyan/work/workspace/HILDA+_LU_2010/hilda_plus_2010_class2_frac_0.083_avg.tif\")\n",
    "pasture_frac_2010 = xr.open_rasterio(\"/home/jovyan/work/workspace/HILDA+_LU_2010/hilda_plus_2010_class3_frac_0.083_avg.tif\")\n",
    "forest_frac_2010 = xr.open_rasterio(\"/home/jovyan/work/workspace/HILDA+_LU_2010/hilda_plus_2010_class4_frac_0.083_avg.tif\")\n",
    "shrub_frac_2010 = xr.open_rasterio(\"/home/jovyan/work/workspace/HILDA+_LU_2010/hilda_plus_2010_class5_frac_0.083_avg.tif\")\n",
    "other_frac_2010 = xr.open_rasterio(\"/home/jovyan/work/workspace/HILDA+_LU_2010/hilda_plus_2010_class6_frac_0.083_avg.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_frac_2010 = urban_frac_2010.sel(y = slice(lat1,lat2), x = slice(lon1,lon2))\n",
    "crop_frac_2010 = crop_frac_2010.sel(y = slice(lat1,lat2), x = slice(lon1,lon2))\n",
    "pasture_frac_2010 = pasture_frac_2010.sel(y = slice(lat1,lat2), x = slice(lon1,lon2))\n",
    "forest_frac_2010 = forest_frac_2010.sel(y = slice(lat1,lat2), x = slice(lon1,lon2))\n",
    "shrub_frac_2010 = shrub_frac_2010.sel(y = slice(lat1,lat2), x = slice(lon1,lon2))\n",
    "other_frac_2010 = other_frac_2010.sel(y = slice(lat1,lat2), x = slice(lon1,lon2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare country map for coordinate selection on land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_use_sum = urban_frac_2010 + crop_frac_2010 + pasture_frac_2010 + forest_frac_2010 + shrub_frac_2010 + other_frac_2010\n",
    "land_coord = ((land_use_sum < 2) & (land_use_sum > 0))\n",
    "land_mask = xr.ones_like(crop_frac_2010)\n",
    "land_mask = land_mask.where((land_use_sum < 2) & (land_use_sum > 0), 0)\n",
    "land_mask = land_mask[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_mask.plot.pcolormesh(figsize=(16,10))\n",
    "plt.title(\"land mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract all land pixels for parameter table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.array(land_mask != 0).flatten()\n",
    "lat = np.array(np.repeat(countries.y, countries.shape[2]))[pos]\n",
    "lon = np.array(np.repeat(countries.x, countries.shape[1]))[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.array(land_mask) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_val = np.array(biomass_2010[0,:,:])\n",
    "biomass_val = biomass_val[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = np.array(land_surface_temperaturemap)[pos]\n",
    "prec = np.array(precipitationmap)[pos]\n",
    "soilevap = np.array(bare_soil_evaporationmap)[pos]\n",
    "albedo = np.array(black_sky_albedomap)[pos]\n",
    "lai = np.array(leaf_area_indexmap)[pos]\n",
    "evap = np.array(evaporationmap)[pos]\n",
    "gpp = np.array(gross_primary_productivitymap)[pos]\n",
    "nee = np.array(net_ecosystem_exchangemap)[pos]\n",
    "rootm = np.array(root_moisturemap)[pos]\n",
    "surfm = np.array(surface_moisturemap)[pos]\n",
    "resp = np.array(terrestrial_ecosystem_respirationmap)[pos]\n",
    "urban = np.array(urban_frac_2010[0,:,:])[pos]\n",
    "crop = np.array(crop_frac_2010[0,:,:])[pos]\n",
    "past = np.array(pasture_frac_2010[0,:,:])[pos]\n",
    "forest = np.array(forest_frac_2010[0,:,:])[pos]\n",
    "shrub = np.array(shrub_frac_2010[0,:,:])[pos]\n",
    "other = np.array(other_frac_2010[0,:,:])[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting up a pandas table for the descriptors and response variables\n",
    "chose different variable ensembles and name the descriptor setting accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0 = pd.DataFrame({\"Lat\":lat, \"Lon\":lon, \"lst\":lst, \"prec\": prec, \"soilevap\": soilevap, \n",
    "            \"albedo\":albedo, \"lai\":lai, \"evap\":evap, \"gpp\":gpp, \"nee\": nee, \"rootm\":rootm, \"surfm\":surfm, \n",
    "             \"resp\":resp, \"LU_urban\": urban, \"LU_crop\": crop, \"LU_past\": past, \"LU_forest\": forest, \"LU_shrub\": shrub, \"LU_other\": other, \"agb\":biomass_val})\n",
    "\n",
    "#dataset0 = pd.DataFrame({\"Lat\":lat, \"Lon\":lon, \"lst\":lst, \"prec\": prec, \"soilevap\": soilevap, \n",
    "#            \"albedo\":albedo, \"lai\":lai, \"evap\":evap, \"gpp\":gpp, \"nee\": nee, \"rootm\":rootm, \"surfm\":surfm, \n",
    "#             \"resp\":resp, \"agb\":biomass_val})\n",
    "\n",
    "#dataset0 = pd.DataFrame({\"Lat\":lat, \"Lon\":lon, \"LU_urban\": urban, \"LU_crop\": crop, \"LU_past\": past, \"LU_forest\": forest, \"LU_shrub\": shrub, \"LU_other\": other, \"agb\":biomass_val})\n",
    "\n",
    "name = \"all-var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset0.dropna()\n",
    "dataset = dataset.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig = dataset #keep a backup of the original dataset. Might be useful.\n",
    "dataset.to_csv('dataset_clean_' + region + '_' + name + '.csv',index=False) #Saving the csv file just for easier visualization of the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range = (0,1)) #Scaling features to a range between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and translating each feature to our chosen range\n",
    "dataset = sc.fit_transform(dataset) \n",
    "dataset = pd.DataFrame(dataset, columns = dataset_orig.columns)\n",
    "dataset_scaled = dataset #Just backup\n",
    "inverse_data = sc.inverse_transform(dataset) #just to make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8,random_state=0) # build training data set (random state is fixing the seed)\n",
    "test_dataset = dataset.drop(train_dataset.index) #  take the rest as test data set\n",
    "train_dataset_orig = dataset_orig.sample(frac=0.8,random_state=0) #just backup\n",
    "test_dataset_orig =  dataset_orig.drop(train_dataset_orig.index) #just backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the original mean\n",
    "sns.set()\n",
    "f, (ax1,ax2) = plt.subplots(2, 1,sharex=True)\n",
    "sns.distplot(train_dataset[\"agb\"],hist=True,kde=False,bins=75,color='darkblue',  ax=ax1, axlabel=False)\n",
    "sns.kdeplot(train_dataset[\"agb\"],bw=0.15,legend=True,color='darkblue', ax=ax2)\n",
    "\n",
    "ax1.set_title('Original  histogram')\n",
    "ax1.legend(['AGB'])\n",
    "ax2.set_title('KDE')\n",
    "ax2.set_xlabel('AGB')\n",
    "ax1.set_ylabel('Count')\n",
    "ax2.set_ylabel('Dist')\n",
    "plt.savefig('histograms_' + region + '.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the overall stats\n",
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop('agb') #because that is what we are trying to predict\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the output from our list of predictors, save training dataset\n",
    "train_dataset.to_csv('train_dataset_' + region + '_' + name + '.csv',index=False) \n",
    "train_labels = train_dataset.pop('agb')\n",
    "#train_dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save testing dataset\n",
    "test_dataset.to_csv('test_dataset_' + region + '_' + name + '.csv',index=False)\n",
    "test_labels = test_dataset.pop('agb')\n",
    "#test_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting up the neural network\n",
    "### some variables for the optimizer SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_val = 0.01 # change the learning rate to adapt optimizing behaviour, learning rate is a hyperparameter / how fast should it learn?\n",
    "# small LR -> long convergence time\n",
    "# large LR -> convergence problems\n",
    "momentum_val = 0  # change the momentum to adapt optimizing behaviour\n",
    "nesterov_val = 'True'\n",
    "#value adapts the optimizing behaviour (rebalancing the direction between momentum and learning rate)\n",
    "decay_val = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    ## how much layers? \n",
    "    # why 64 units? increase in number of units in hidden layers increases the chances of overfitting\n",
    "    # activation function: rectified linear unit (relu)\n",
    "    # insert layers of dropout in between (regularization to avoid overfitting)\n",
    "    # how deep should the NN be? overfitting chances are increased!?\n",
    "    \n",
    "    layers.Dense(64,kernel_initializer='normal',activation=tf.nn.relu,input_shape=[len(train_dataset.keys())]),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation=tf.nn.relu),\n",
    "    layers.Dropout(0.2), \n",
    "    layers.Dense(1,kernel_initializer='normal',activation='relu') \n",
    "    # final layer has only one unit (change it if you do multi-output classification)\n",
    "  ])\n",
    "  \n",
    "  optimizer = tf.keras.optimizers.SGD(lr=lr_val, momentum=momentum_val, \n",
    "                                      decay=decay_val, nesterov=nesterov_val) # many other options for optmizer\n",
    "  model.compile(loss='mean_squared_error',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mean_absolute_error', 'mean_squared_error']) \n",
    "    #When dealing with classification, 'accuracy' is very useful as well\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take 10 samples from training dataset for quick test\n",
    "example_batch = test_dataset[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, \n",
    "                                           patience=100, mode='auto', baseline=None, \n",
    "                                           restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model\n",
    "this takes a while, depending on the size of the dataset, the number of EPOCHS, and the settings of the early_stop function (patience, min_delta, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000 # nr of iteration for the network before stopping (ultimate stop...could be 5000, but takes a long time)\n",
    "\n",
    "history = model.fit(\n",
    "  train_dataset, train_labels,\n",
    "  epochs=EPOCHS, \n",
    "  validation_split=0.1, # split the data set in 90/10\n",
    "  shuffle=True, verbose=2,\n",
    "  callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the learning curve/ the progress of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error [Mean Conc. N]')\n",
    "  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "           label = 'Val Error')\n",
    "  #plt.ylim([0,1])\n",
    "  plt.legend()\n",
    "  plt.savefig('mean_asb_error_lr' + str(lr_val) + '_moment' + str(momentum_val) + '_nest' + str(nesterov_val) + \"_\" + region + \"_\" + name + '.png', bbox_inches='tight')\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Square Error [$(Mean Conc.)^2$]')\n",
    "  plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "           label = 'Val Error')\n",
    "  #plt.ylim([0,3])\n",
    "  plt.legend()\n",
    "  plt.savefig('mean_sq_error_lr' + str(lr_val) + '_moment' + str(momentum_val) + '_nest' + str(nesterov_val) + \"_\" + region + \"_\" + name + '.png', bbox_inches='tight')\n",
    "  # plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time for a real prediction on the test data set\n",
    "print(\"predict the model on the test data set...\")\n",
    "f, (ax1,ax2) = plt.subplots(1,2, sharey=True)\n",
    "test_predictions = model.predict(test_dataset).flatten()\n",
    "r = r2_score(test_labels, test_predictions)\n",
    "ax1.scatter(test_labels, test_predictions,alpha=0.5, label='$R^2$ = %.3f' % (r))\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax1.set_xlabel('True Values [Mean Conc.]')\n",
    "ax1.set_ylabel('Predictions [Mean Conc.]')\n",
    "ax1.axis('equal')\n",
    "ax1.axis('square')\n",
    "ax1.set_xlim([0,1])\n",
    "ax1.set_ylim([0,1])\n",
    "_ = ax1.plot([-100, 100], [-100, 100], 'r:')\n",
    "ax1.set_title('Test dataset')\n",
    "f.set_figheight(30)\n",
    "f.set_figwidth(10)\n",
    "\n",
    "# Predict for the whole dataset\n",
    "print(\"predict the model on the whole data set...\")\n",
    "dataset_labels = dataset.pop('agb')\n",
    "dataset = dataset\n",
    "dataset_predictions = model.predict(dataset).flatten()\n",
    "r = r2_score(dataset_labels, dataset_predictions)\n",
    "ax2.scatter(dataset_labels, dataset_predictions, alpha=0.5, label='$R^2$ = %.3f' % (r))\n",
    "ax2.legend(loc=\"upper left\")\n",
    "ax2.set_xlabel('True Values [Mean Conc.]')\n",
    "ax2.set_ylabel('Predictions [Mean Conc.]')\n",
    "ax2.axis('equal')\n",
    "ax2.axis('square')\n",
    "ax2.set_xlim([0,1])\n",
    "ax2.set_ylim([0,1])\n",
    "_ = ax2.plot([-100, 100], [-100, 100], 'r:')\n",
    "ax2.set_title('Whole dataset')\n",
    "# plt.show()\n",
    "plt.savefig('R_scaled_lr' + str(lr_val) + '_moment' + str(momentum_val) + '_nest' + str(nesterov_val) + \"_\" + region + \"_\" + name + '.png', bbox_inches='tight')\n",
    "# plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undo scale step\n",
    "# Test data set\n",
    "test_dataset['agb'] = test_predictions\n",
    "inverse_data = sc.inverse_transform(test_dataset)\n",
    "inverse_data = pd.DataFrame(inverse_data, columns = dataset_orig.columns)\n",
    "test_predictions = inverse_data.pop('agb')\n",
    "test_labels = test_dataset_orig.pop('agb')\n",
    "\n",
    "# Whole dataset\n",
    "dataset['agb'] = dataset_predictions\n",
    "inverse_data = sc.inverse_transform(dataset)\n",
    "inverse_data = pd.DataFrame(inverse_data, columns = dataset.columns)\n",
    "dataset_predictions = inverse_data.pop('agb')\n",
    "dataset_labels = dataset_orig.pop('agb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Rsquared - scatter plot\n",
    "\n",
    "f, (ax1,ax2) = plt.subplots(1,2, sharey=True)\n",
    "\n",
    "# Test dataset\n",
    "r = r2_score(test_labels, test_predictions)\n",
    "ax1.scatter(test_labels, test_predictions, alpha=0.5, label='$R^2$ = %.3f' % (r))\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax1.set_xlabel('True Values [Mean Conc.]')\n",
    "ax1.set_ylabel('Predictions [Mean Conc.]')\n",
    "ax1.axis('equal')\n",
    "ax1.axis('square')\n",
    "ax1.set_xlim([0,max(test_labels.max(), test_predictions.max())])\n",
    "ax1.set_ylim([0,max(test_labels.max(), test_predictions.max())])\n",
    "_ = ax1.plot([-max(test_labels.max(), test_predictions.max()), max(test_labels.max(), test_predictions.max())], [-max(test_labels.max(), test_predictions.max()), max(test_labels.max(), test_predictions.max())], 'r:')\n",
    "ax1.set_title('Test dataset')\n",
    "f.set_figheight(30)\n",
    "f.set_figwidth(10)\n",
    "#plt.show()\n",
    "\n",
    "# Whole dataset\n",
    "r = r2_score(dataset_labels, dataset_predictions)\n",
    "ax2.scatter(dataset_labels, dataset_predictions, alpha=0.5, label='$R^2$ = %.3f' % (r))\n",
    "ax2.legend(loc=\"upper left\")\n",
    "ax2.set_xlabel('True Values [Mean Conc.]')\n",
    "ax2.set_ylabel('Predictions [Mean Conc.]')\n",
    "ax2.axis('equal')\n",
    "ax2.axis('square')\n",
    "ax2.set_xlim([0,max(dataset_labels.max(), dataset_predictions.max())])\n",
    "ax2.set_ylim([0,max(dataset_labels.max(), dataset_predictions.max())])\n",
    "_ = ax2.plot([-max(dataset_labels.max(), dataset_predictions.max()), max(dataset_labels.max(), dataset_predictions.max())], [-max(dataset_labels.max(), dataset_predictions.max()), max(dataset_labels.max(), dataset_predictions.max())], 'r:')\n",
    "ax2.set_title('Whole dataset')\n",
    "# plt.show()\n",
    "plt.savefig('R_unscaled_lr' + str(lr_val) + '_moment' + str(momentum_val) + '_nest' + str(nesterov_val) + \"_\" + region + \"_\" + name + '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGB_results = pd.DataFrame({\"Lat\":inverse_data[\"Lat\"], \"Lon\":inverse_data[\"Lat\"], \"AGB\": dataset_predictions})\n",
    "pos1 = dataset0.notna().all(axis = 1)\n",
    "out_biomass = xr.zeros_like(biomass_2010)\n",
    "out_biomass_vals = np.array(out_biomass[0,:,:])\n",
    "out_biomass_vals_sub = out_biomass_vals[pos]\n",
    "out_biomass_vals_sub[pos1] = AGB_results[\"AGB\"]\n",
    "out_biomass_vals_sub[pos1== False] = np.nan\n",
    "out_biomass_vals[pos] = out_biomass_vals_sub\n",
    "out_biomass_vals.shape\n",
    "out_biomass[0,:,:] = out_biomass_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot map of predicted biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_biomass[0,:,:].plot.pcolormesh(figsize=(16,10))\n",
    "plt.title('Predicted AGB')\n",
    "plt.ylabel('Lat')\n",
    "plt.xlabel('Lon')\n",
    "plt.savefig(\"predicted_biomass_2010_map_\" + region + \"_\" + name + \".png\",dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot difference between predicted and reference biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_diff = out_biomass - biomass_2010\n",
    "biomass_diff = biomass_diff * (land_mask == 1) * (pos)\n",
    "biomass_diff_vals = np.array(biomass_diff[0,:,:])\n",
    "biomass_diff_sub = biomass_diff_vals[pos]\n",
    "biomass_diff_sub[pos1==False] = 0\n",
    "biomass_diff_vals[pos] = biomass_diff_sub\n",
    "biomass_diff[0,:,:] = biomass_diff_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_diff[0,:,:].plot.pcolormesh(figsize=(16,10))\n",
    "plt.title('Predicted - Reference AGB')\n",
    "plt.ylabel('Lat')\n",
    "plt.xlabel('Lon')\n",
    "plt.savefig(\"diff_biomass_2010_map_\" + region + \"_\" + name + \".png\",dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
